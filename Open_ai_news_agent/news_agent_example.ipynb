{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a26d75",
   "metadata": {},
   "source": [
    "### Testing an agent for downloading news from GNews API and summarizing them for user convenience\n",
    "First, we install OpenAI libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d265ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install -U \"openai==2.2.0\" \"openai-agents==0.1.0\" python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4ea71",
   "metadata": {},
   "source": [
    "Then, we import libraries required for our agent and also config with environment variables (stored in the .env file in the same directory as this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3bc43de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, asyncio\n",
    "\n",
    "#libraries required for the agent\n",
    "from openai import OpenAI\n",
    "from agents.agent import Agent\n",
    "from agents.run import Runner\n",
    "from agents.tool import function_tool\n",
    "\n",
    "#libraries required for the functions\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "#loading environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ccd17",
   "metadata": {},
   "source": [
    "For manipulating the news, we create a class which will have all the fields required for the news processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ce844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewsArticle:\n",
    "    \"\"\"Represents a news article with all relevant metadata.\"\"\"\n",
    "    \n",
    "    title: str\n",
    "    content: str\n",
    "    url: str\n",
    "    source: str\n",
    "    published_at: str\n",
    "    language: str\n",
    "    category: str\n",
    "    summary: Optional[str] = None\n",
    "    english_summary: Optional[str] = None\n",
    "    english_title: Optional[str] = None\n",
    "    relevance_score: Optional[float] = None\n",
    "    quality_score: Optional[float] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate and clean data after initialization.\"\"\"\n",
    "        if not self.title.strip():\n",
    "            raise ValueError(\"Title cannot be empty\")\n",
    "        if not self.content.strip():\n",
    "            raise ValueError(\"Content cannot be empty\")\n",
    "        if not self.url.strip():\n",
    "            raise ValueError(\"URL cannot be empty\")\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert article to dictionary for serialization.\"\"\"\n",
    "        return {\n",
    "            'title': self.title,\n",
    "            'content': self.content,\n",
    "            'url': self.url,\n",
    "            'source': self.source,\n",
    "            'published_at': self.published_at,\n",
    "            'language': self.language,\n",
    "            'category': self.category,\n",
    "            'summary': self.summary,\n",
    "            'english_summary': self.english_summary,\n",
    "            'english_title': self.english_title,\n",
    "            'relevance_score': self.relevance_score,\n",
    "            'quality_score': self.quality_score\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict) -> 'NewsArticle':\n",
    "        \"\"\"Create article from dictionary.\"\"\"\n",
    "        return cls(**data)\n",
    "    \n",
    "    def get_display_title(self) -> str:\n",
    "        \"\"\"Get the best available title (English preferred).\"\"\"\n",
    "        return self.english_title or self.title\n",
    "    \n",
    "    def get_display_summary(self) -> str:\n",
    "        \"\"\"Get the best available summary (English preferred).\"\"\"\n",
    "        return self.english_summary or self.summary or self.content[:200] + \"...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d5a9d1",
   "metadata": {},
   "source": [
    "Then, we create a config class for an agent.\n",
    "\n",
    "The agent will be able to query GNews for articles in multiple languages and from different countries to maintain diverse source condition. And the class will store both API keys required for fetching and processing the news, and also the diversity factors information, like country of news origin and languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentConfig:\n",
    "    \"\"\"Configuration for the news agent.\"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    openai_api_key: str\n",
    "    gnews_api_key: str\n",
    "    \n",
    "    # Processing Configuration\n",
    "    max_articles_per_source: int = 2\n",
    "    target_total_articles: int = 15\n",
    "    min_sources: int = 5\n",
    "    \n",
    "    # Diversity Configuration\n",
    "    countries: list[str] = None\n",
    "    languages: list[str] = None\n",
    "    academic_sources_enabled: bool = True\n",
    "    \n",
    "    # AI Configuration\n",
    "    model: str = \"gpt-4o-mini\"\n",
    "    max_tokens: int = 200\n",
    "    temperature: float = 0.3\n",
    "    \n",
    "    # Rate Limiting\n",
    "    request_delay: float = 1.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set default values if not provided.\"\"\"\n",
    "        if self.countries is None:\n",
    "            self.countries = ['us', 'gb', 'ca', 'ru', 'in', 'de', 'fr', 'jp']\n",
    "        if self.languages is None:\n",
    "            self.languages = ['en', 'ru']\n",
    "    \n",
    "    def to_dict(self) -> dict[str, any]:\n",
    "        \"\"\"Convert config to dictionary.\"\"\"\n",
    "        return {\n",
    "            'openai_api_key': self.openai_api_key,\n",
    "            'gnews_api_key': self.gnews_api_key,\n",
    "            'max_articles_per_source': self.max_articles_per_source,\n",
    "            'target_total_articles': self.target_total_articles,\n",
    "            'min_sources': self.min_sources,\n",
    "            'countries': self.countries,\n",
    "            'languages': self.languages,\n",
    "            'academic_sources_enabled': self.academic_sources_enabled,\n",
    "            'model': self.model,\n",
    "            'max_tokens': self.max_tokens,\n",
    "            'temperature': self.temperature,\n",
    "            'request_delay': self.request_delay,\n",
    "            'academic_delay': self.academic_delay\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict[str, any]) -> 'AgentConfig':\n",
    "        \"\"\"Create config from dictionary.\"\"\"\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b5789",
   "metadata": {},
   "source": [
    "We then populate the config with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0645a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_from_env():\n",
    "    \"\"\"Create configuration from environment variables.\"\"\"\n",
    "    try:\n",
    "        return AgentConfig(\n",
    "            openai_api_key=os.getenv('OPENAI_API_KEY', ''),\n",
    "            gnews_api_key=os.getenv('GNEWS_API_KEY', '')\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating config from environment: {e}\")\n",
    "        return None\n",
    "\n",
    "config = create_config_from_env()\n",
    "gnews_base_url = 'https://gnews.io/api/v4'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51998302",
   "metadata": {},
   "source": [
    "The agent will need to have the access to multiple functions which fetch the data from GNews using agent config, and also processing the news (like summarization or ensuring diversity). The full list of functions available for agent is:\n",
    "\n",
    "- fetch_news_from_gnews. Function fetches news from API of GNews. \n",
    "- fetch_diverse_news_sources. Fetches enough articles from each source, and also for each language and country. \n",
    "- filter_articles_by_relevance. Calculate relevance score based on count of relevant words to the topic in the title and content and ranges the articles by relevance. \n",
    "- ensure_source_diversity. There is another setting in the agent config of how many articles we need to have from each source for diversity, and it is related to number of articles filtered by relevance. \n",
    "- get_source_statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d44d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def fetch_news_from_gnews(\n",
    "     \n",
    "    query: str, \n",
    "    max_articles: int = 10, \n",
    "    country: str = 'us', \n",
    "    language: str = 'en'\n",
    ") -> list[dict[str, any]]:\n",
    "    \"\"\"Fetch news articles from GNews API.\"\"\"\n",
    "    try:\n",
    "        url = f\"{gnews_base_url}/search\"\n",
    "        params = {\n",
    "            'token': config.gnews_api_key,\n",
    "            'q': query,\n",
    "            'country': country,\n",
    "            'lang': language,\n",
    "            'max': max_articles,\n",
    "            'expand': 'content'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        \n",
    "        print(f\"Fetched {len(articles)} articles for query '{query}' from {country} ({language})\")\n",
    "        return articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "@function_tool\n",
    "def fetch_diverse_news_sources( query: str) -> list[NewsArticle]:\n",
    "    \"\"\"Fetch news from diverse sources to ensure variety.\"\"\"\n",
    "    all_articles = []\n",
    "    source_counts = Counter()\n",
    "    \n",
    "    # Create combinations of countries and languages for diversity\n",
    "    combinations = []\n",
    "    for country in config.countries:\n",
    "        for language in config.languages:\n",
    "            combinations.append((country, language))\n",
    "    \n",
    "    random.shuffle(combinations)  # Randomize order\n",
    "    \n",
    "    for country, language in combinations:\n",
    "        if len(all_articles) >= config.target_total_articles:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            articles = fetch_news_from_gnews(\n",
    "                query, \n",
    "                max_articles=3, \n",
    "                country=country, \n",
    "                language=language\n",
    "            )\n",
    "            \n",
    "            for article in articles:\n",
    "                source_name = article.get('source', {}).get('name', 'Unknown')\n",
    "                \n",
    "                # Check if we already have enough from this source\n",
    "                if source_counts[source_name] >= config.max_articles_per_source:\n",
    "                    continue\n",
    "                    \n",
    "                # Create NewsArticle object\n",
    "                news_article = NewsArticle(\n",
    "                    title=article.get('title', ''),\n",
    "                    content=article.get('content', '') or article.get('description', ''),\n",
    "                    url=article.get('url', ''),\n",
    "                    source=source_name,\n",
    "                    published_at=article.get('publishedAt', ''),\n",
    "                    language=language,\n",
    "                    category='news'\n",
    "                )\n",
    "                \n",
    "                all_articles.append(news_article)\n",
    "                source_counts[source_name] += 1\n",
    "                \n",
    "            # Add delay to avoid rate limiting\n",
    "            time.sleep(config.request_delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching from {country} ({language}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTotal articles collected: {len(all_articles)}\")\n",
    "    print(f\"Unique sources: {len(source_counts)}\")\n",
    "    print(f\"Source distribution: {dict(source_counts)}\")\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "@function_tool\n",
    "def filter_articles_by_relevance(\n",
    "     \n",
    "    articles: list[NewsArticle], \n",
    "    topic: str,\n",
    "    min_relevance_score: float = 0.4\n",
    ") -> list[NewsArticle]:\n",
    "    \"\"\"Filter articles based on relevance to the topic.\"\"\"\n",
    "    relevant_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        # Simple relevance check based on title and content\n",
    "        title_lower = article.title.lower()\n",
    "        content_lower = article.content.lower()\n",
    "        topic_lower = topic.lower()\n",
    "        \n",
    "        # Calculate simple relevance score\n",
    "        title_score = sum(1 for word in topic_lower.split() if word in title_lower)\n",
    "        content_score = sum(1 for word in topic_lower.split() if word in content_lower)\n",
    "        \n",
    "        relevance_score = (title_score * 0.7 + content_score * 0.3) / len(topic_lower.split())\n",
    "        \n",
    "        if relevance_score >= min_relevance_score:\n",
    "            article.relevance_score = relevance_score\n",
    "            relevant_articles.append(article)\n",
    "    \n",
    "    print(f\"Filtered {len(articles)} articles to {len(relevant_articles)} relevant articles\")\n",
    "    return relevant_articles\n",
    "\n",
    "@function_tool\n",
    "def ensure_source_diversity( articles: list[NewsArticle]) -> list[NewsArticle]:\n",
    "    \"\"\"Ensure source diversity in the article collection.\"\"\"\n",
    "    if len(articles) <= config.target_total_articles:\n",
    "        return articles\n",
    "    \n",
    "    source_counts = Counter()\n",
    "    diverse_articles = []\n",
    "    \n",
    "    # Sort articles by relevance score (if available)\n",
    "    sorted_articles = sorted(\n",
    "        articles, \n",
    "        key=lambda x: x.relevance_score or 0, \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for article in sorted_articles:\n",
    "        source_name = article.source\n",
    "        \n",
    "        # Check if we need more diversity\n",
    "        if source_counts[source_name] < config.max_articles_per_source:\n",
    "            diverse_articles.append(article)\n",
    "            source_counts[source_name] += 1\n",
    "        \n",
    "        if len(diverse_articles) >= config.target_total_articles:\n",
    "            break\n",
    "    \n",
    "    print(f\"Ensured diversity: {len(diverse_articles)} articles from {len(source_counts)} sources\")\n",
    "    return diverse_articles\n",
    "\n",
    "@function_tool\n",
    "def get_source_statistics( articles: list[NewsArticle]) -> dict[str, any]:\n",
    "    \"\"\"Get statistics about article sources.\"\"\"\n",
    "    if not articles:\n",
    "        return {}\n",
    "    \n",
    "    source_counts = Counter(article.source for article in articles)\n",
    "    language_counts = Counter(article.language for article in articles)\n",
    "    category_counts = Counter(article.category for article in articles)\n",
    "    \n",
    "    return {\n",
    "        'total_articles': len(articles),\n",
    "        'unique_sources': len(source_counts),\n",
    "        'source_distribution': dict(source_counts),\n",
    "        'language_distribution': dict(language_counts),\n",
    "        'category_distribution': dict(category_counts),\n",
    "        'academic_articles': sum(1 for article in articles if article.is_academic),\n",
    "        'news_articles': sum(1 for article in articles if not article.is_academic)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f640c0",
   "metadata": {},
   "source": [
    "For the functions, related to processing of the news content (summarization, translation, evaluating relevance) - we need to create a client to OpenAI. Then, we describe the tools used for the functions above. Some of the functions (like the one for relevance) is an LLM alternative to the finding the relevance by the word count (like the function in the block above). Somebody who makes the final agent instruction need to maintain balance between the filtering efficiency and tokens count, chosing between those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66ae5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=config.openai_api_key)\n",
    "\n",
    "@function_tool\n",
    "def generate_summary( text: str, language: str = 'en', topic: str = None) -> str:\n",
    "    \"\"\"Generate summary using OpenAI.\"\"\"\n",
    "    try:\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            return \"Insufficient content for summary.\"\n",
    "        \n",
    "        # Truncate if too long\n",
    "        max_length = 3000\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length] + \"...\"\n",
    "        \n",
    "        system_prompt = f\"You are a news summarizer. Create a concise, informative summary in {language}. Focus on key facts and main points. Keep it under 150 words.\"\n",
    "        \n",
    "        if topic:\n",
    "            system_prompt += f\" Focus on aspects relevant to '{topic}'.\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Summarize this content:\\n\\n{text}\"\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=config.max_tokens,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "        return text[:200] + \"...\" if len(text) > 200 else text\n",
    "\n",
    "@function_tool\n",
    "def translate_to_english( text: str, topic: str = None) -> str:\n",
    "    \"\"\"Translate text to english using OpenAI.\"\"\"\n",
    "    try:\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return text\n",
    "        \n",
    "        system_prompt = \"You are a professional translator. Translate the following text to English. Maintain the original meaning and tone. If the text is already in English, return it as is.\"\n",
    "        \n",
    "        if topic:\n",
    "            system_prompt += f\" Pay special attention to terminology related to '{topic}'.\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Translate to english: {text}\"\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error translating to english: {e}\")\n",
    "        return text\n",
    "\n",
    "@function_tool\n",
    "def evaluate_article_relevance( article: NewsArticle, topic: str) -> dict[str, any]:\n",
    "    \"\"\"Evaluate article relevance using AI.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"You are a news relevance evaluator. Determine if an article is relevant to the given topic. Consider:\n",
    "                    1. Direct relevance to the topic\n",
    "                    2. Quality of content\n",
    "                    3. Recency\n",
    "                    4. Source credibility\n",
    "                    \n",
    "                    Return a JSON response with:\n",
    "                    - relevance_score: float (0-1)\n",
    "                    - is_relevant: boolean\n",
    "                    - reasoning: string\n",
    "                    - quality_score: float (0-1)\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Topic: '{topic}'\\nArticle: '{article.title}'\\nContent: '{article.content[:500]}...'\\n\\nIs this article relevant?\"\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        evaluation_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        try:\n",
    "            import json\n",
    "            evaluation = json.loads(evaluation_text)\n",
    "        except:\n",
    "            # Fallback evaluation\n",
    "            evaluation = {\n",
    "                \"relevance_score\": 0.7,\n",
    "                \"is_relevant\": True,\n",
    "                \"reasoning\": \"Article appears relevant to topic\",\n",
    "                \"quality_score\": 0.6\n",
    "            }\n",
    "        \n",
    "        return evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in article evaluation: {e}\")\n",
    "        return {\n",
    "            \"relevance_score\": 0.5, \n",
    "            \"is_relevant\": True, \n",
    "            \"reasoning\": f\"Error in evaluation: {e}\",\n",
    "            \"quality_score\": 0.5\n",
    "        }\n",
    "\n",
    "@function_tool\n",
    "def analyze_topic( topic: str) -> dict[str, any]:\n",
    "    \"\"\"Analyze topic and provide processing recommendations.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"You are an expert news analyst. Analyze the given topic and determine the best strategy for news processing. Consider:\n",
    "                    1. What countries/regions would have the most relevant news?\n",
    "                    2. What languages should be prioritized?\n",
    "                    3. Are there academic sources that would be relevant?\n",
    "                    4. What's the expected diversity of sources?\n",
    "                    \n",
    "                    Return a JSON response with your analysis and recommendations.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Analyze this news topic and recommend a processing strategy: '{topic}'\"\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        analysis_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        try:\n",
    "            import json\n",
    "            analysis = json.loads(analysis_text)\n",
    "        except:\n",
    "            # Fallback analysis\n",
    "            analysis = {\n",
    "                \"topic\": topic,\n",
    "                \"recommended_countries\": [\"us\", \"gb\", \"in\", \"ru\", \"cn\"],\n",
    "                \"recommended_languages\": [\"en\", \"ru\"],\n",
    "                \"academic_relevance\": \"medium\",\n",
    "                \"expected_diversity\": \"high\",\n",
    "                \"strategy\": \"comprehensive\"\n",
    "            }\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in topic analysis: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "@function_tool\n",
    "def merge_related_articles( articles: list[NewsArticle]) -> list[NewsArticle]:\n",
    "    \"\"\"Merge related articles using AI.\"\"\"\n",
    "    if len(articles) <= 1:\n",
    "        return articles\n",
    "    \n",
    "    try:\n",
    "        # Group articles by similarity (simplified approach)\n",
    "        article_groups = []\n",
    "        \n",
    "        for article in articles:\n",
    "            # Simple similarity check based on title keywords\n",
    "            added_to_group = False\n",
    "            for group in article_groups:\n",
    "                if any(keyword in article.title.lower() for keyword in group[0].title.lower().split()[:3]):\n",
    "                    group.append(article)\n",
    "                    added_to_group = True\n",
    "                    break\n",
    "            \n",
    "            if not added_to_group:\n",
    "                article_groups.append([article])\n",
    "        \n",
    "        merged_articles = []\n",
    "        \n",
    "        for group in article_groups:\n",
    "            if len(group) == 1:\n",
    "                merged_articles.append(group[0])\n",
    "            else:\n",
    "                # Merge multiple articles\n",
    "                primary_article = group[0]\n",
    "                supplementary_articles = group[1:]\n",
    "                \n",
    "                # Create merged content\n",
    "                merged_content = f\"{primary_article.content}\\n\\n\"\n",
    "                merged_content += \"Additional sources:\\n\"\n",
    "                for article in supplementary_articles:\n",
    "                    merged_content += f\"- {article.title} ({article.source})\\n\"\n",
    "                \n",
    "                primary_article.content = merged_content\n",
    "                merged_articles.append(primary_article)\n",
    "        \n",
    "        return merged_articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error merging articles: {e}\")\n",
    "        return articles\n",
    "\n",
    "@function_tool\n",
    "def process_articles_batch(\n",
    "     \n",
    "    articles: list[NewsArticle], \n",
    "    topic: str\n",
    ") -> list[NewsArticle]:\n",
    "    \"\"\"Process a batch of articles with AI tools.\"\"\"\n",
    "    processed_articles = []\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Generate summary\n",
    "            if not article.summary:\n",
    "                article.summary = generate_summary(\n",
    "                    article.content, \n",
    "                    article.language, \n",
    "                    topic\n",
    "                )\n",
    "            \n",
    "            # Translate to english\n",
    "            if not article.english_title:\n",
    "                article.english_title = translate_to_english(article.title, topic)\n",
    "            if not article.english_summary:\n",
    "                article.english_summary = translate_to_english(article.summary, topic)\n",
    "            \n",
    "            processed_articles.append(article)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article '{article.title}': {e}\")\n",
    "            processed_articles.append(article)  # Add unprocessed article\n",
    "    \n",
    "    return processed_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65615de9",
   "metadata": {},
   "source": [
    "Finally, we can create an agent, give it an instruction on news fetching and processing and list the tools the agent can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"News agent\",\n",
    "    instructions=\"\"\"You are an intelligent news processing agent. Your job is to help users get comprehensive news coverage on any topic.\n",
    "\n",
    "You have access to the following tools:\n",
    "- analyze_topic: Analyze a topic and determine processing strategy\n",
    "- fetch_news_articles: Get news from diverse sources\n",
    "- summarize_content: Generate AI summaries\n",
    "- translate_to_english: Translate content to English\n",
    "- evaluate_relevance: Check if content is relevant\n",
    "- merge_related_articles: Combine related articles\n",
    "\n",
    "Your goal is to provide comprehensive, diverse, and high-quality news coverage. Use your tools intelligently to:\n",
    "1. Understand what the user wants\n",
    "2. Gather information from multiple sources\n",
    "3. Ensure diversity in sources and perspectives\n",
    "4. Process and enhance the content\n",
    "5. Present results in a useful format\n",
    "\n",
    "Be strategic about which tools to use and in what order. You can call multiple tools, combine results, and make decisions based on what you find.\"\"\",\n",
    "    tools=[fetch_news_from_gnews, fetch_diverse_news_sources, filter_articles_by_relevance, ensure_source_diversity, get_source_statistics, generate_summary, translate_to_english, evaluate_article_relevance,analyze_topic, merge_related_articles, process_articles_batch],\n",
    ")\n",
    "\n",
    "#Requesting the agent to give a comprehensive coverage on some topic:\n",
    "async def main():\n",
    "    result = await Runner.run(agent, input=\"Recent Meta glasses release\")\n",
    "    print(result.final_output)\n",
    "   \n",
    "\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
